{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title:  \"Validating Hyperparameters\"\n",
        "author: <br><br><br><br><span style=\"font-family:perpetua; font-variant:small-caps; color:#404040;\">Kerry Back</span><br><br><img src=\"RiceBusiness-transparent-logo-sm.png\"  height=80>\n",
        "execute:\n",
        "  echo: false\n",
        "  jupyter: python3\n",
        "  cache: true\n",
        "format: \n",
        "  revealjs:\n",
        "    highlight-style: monokai\n",
        "    code-fold: true\n",
        "    scrollable: true\n",
        "    slide-number: true\n",
        "    preview-links: true\n",
        "    self-contained: true\n",
        "    controls: true\n",
        "    transition: fade\n",
        "    theme: [solarized, 721.scss]\n",
        "    incremental: true\n",
        "---"
      ],
      "id": "947738cb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "\n",
        "- The max depth of trees in a forest and the number of trees are called hyperparameters.\n",
        "- Hyperparameter means that they are specified ex ante rather than calculated through fitting.\n",
        "- The hidden layer sizes in a neural net are also hyperparameters.\n",
        "\n",
        "## Overfitting\n",
        "\n",
        "- Hyperparameters control how complex the model is.\n",
        "- More complex models will better fit the training data.\n",
        "- But we risk overfitting.\n",
        "  - Overfitting means fitting our model to random peculiarities of the training data.\n",
        "  - An overfit model will not work well on new data.\n",
        "- So more complexity is not necessarily better.\n",
        "\n",
        "## Validation\n",
        "\n",
        "- Reserve some data called validation data.\n",
        "- Train with different hyperparameters on training data that does not include validation data.\n",
        "- Choose hyperparameters that perform best on validation data.\n",
        "\n",
        "## Cross validation\n",
        "\n",
        "- Split data into, for example, 3 sets of equal size, say A, B, and C.\n",
        "- Train on A $\\cup$ B, assess performance on C\n",
        "- Train on A $\\cup$ C, assess performance on B\n",
        "- Train on B $\\cup$ C, assess performance on A\n",
        "- Choose hyperparameters with best average performance on A, B, and C.\n",
        "\n",
        "## Grid Search CV\n",
        "\n",
        ". . .\n",
        "\n",
        "```p\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "```\n",
        "\n",
        "- Pass a model or pipeline to GridSearchCV without specifying the hyperparameters.\n",
        "- Pass a set (\"grid\") of hyperparameters to evaluate.\n",
        "- Fit the GridSearchCV.\n",
        "\n",
        "## Everything in one step\n",
        "\n",
        "Fitting GridSearchCV does all of the following:\n",
        "\n",
        "- Randomly choose the subsets A, B, and C (default is 5 subsets rather than 3).\n",
        "- Fit the model or pipeline on training sets and evaluate on validation sets.\n",
        "- Choose hyperparameters with best average performance.\n",
        "- Refit the model on the entire dataset using the best hyperparameters.\n",
        "\n",
        "## Random forest example\n",
        "\n",
        "- roeq, mom12m, and rnk for 2021-01 as before\n",
        "- Define model without specifying max depth.\n",
        "\n",
        ". . .\n",
        "\n",
        "```p\n",
        "model = RandomForestRegressor(\n",
        "  random_state=0\n",
        ")\n",
        "```\n",
        "\n",
        "- Define pipeline as before\n",
        "\n",
        ". . .\n",
        "\n",
        "```p\n",
        "pipe = make_pipeline(\n",
        "  transform,\n",
        "  poly,\n",
        "  transform,\n",
        "  model\n",
        ")\n",
        "```\n",
        "\n",
        "## Define parameters to evaluate\n",
        "\n",
        "\n",
        "- Example: evaluate depths of 4, 6, and 8. \n",
        "- We have to specify what part of the pipeline that the hyperparameters belong to.\n",
        "- Name in lowercase.  Double underscore between name and parameter name.\n",
        "\n",
        ". . .\n",
        "\n",
        "```p\n",
        "param_grid = {\n",
        "    \"randomforestregressor__max_depth\": \n",
        "    [4, 6, 8]\n",
        "}\n",
        "```\n",
        "\n",
        "## Fit and save\n",
        "\n",
        ". . .\n",
        "\n",
        "```{.p code-line-numbers=\"1-4|6-7|9|10\"}\n",
        "cv = GridSearchCV(\n",
        "  pipe, \n",
        "  param_grid=param_grid\n",
        ")\n",
        "\n",
        "X = data[[\"roeq\", \"mom12m\"]]\n",
        "y = data[\"rnk\"]\n",
        "\n",
        "cv.fit(X, y)\n",
        "dump(cv, \"forest2.net\")\n",
        "```\n",
        "\n",
        ". . .\n",
        "\n",
        "<br>\n",
        "Later:\n",
        "\n",
        ". . .\n",
        "\n",
        "```p\n",
        "forest = load(\"forest2.net\")\n",
        "```\n",
        "\n",
        "#\n"
      ],
      "id": "11b264e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sqlalchemy import create_engine\n",
        "import pymssql\n",
        "import pandas as pd\n",
        "\n",
        "server = \"mssql-82792-0.cloudclusters.net:16272\"\n",
        "username = \"user\"\n",
        "password = \"RiceOwls1912\" # paste password between quote marks\n",
        "database = \"ghz\"\n",
        "\n",
        "string = \"mssql+pymssql://\" + username + \":\" + password + \"@\" + server + \"/\" + database\n",
        "\n",
        "conn = create_engine(string).connect()\n",
        "\n",
        "data = pd.read_sql(\n",
        "    \"\"\"\n",
        "    select ticker, date, ret, roeq, mom12m\n",
        "    from data\n",
        "    where date='2021-01'\n",
        "    \"\"\", \n",
        "    conn\n",
        ")\n",
        "data = data.dropna()\n",
        "data['rnk'] = data.ret.rank(pct=True)"
      ],
      "id": "d8571948",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#\n"
      ],
      "id": "0b5b6984"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "transform = QuantileTransformer(\n",
        "    output_distribution=\"normal\"\n",
        ")\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "model = RandomForestRegressor(\n",
        "   random_state=0\n",
        ")\n",
        "pipe = make_pipeline(\n",
        "  transform, \n",
        "  poly,\n",
        "  model\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\n",
        "    \"randomforestregressor__max_depth\": [4, 6, 8]\n",
        "}\n",
        "\n",
        "cv = GridSearchCV(\n",
        "  pipe, \n",
        "  param_grid=param_grid\n",
        ")\n",
        "\n",
        "X = data[[\"roeq\", \"mom12m\"]]\n",
        "y = data[\"rnk\"]\n",
        "\n",
        "cv.fit(X, y)\n",
        "\n",
        "from joblib import dump\n",
        "dump(cv, \"forest2.net\")"
      ],
      "id": "ef91e76f",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}